# -*- coding: utf-8 -*-
"""apml_combined.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bzJWnmr69wGUn8JPTXa0Jp7cUz0w4vx2
"""

#!pip install dipy

# Standard library: general utilities
import inspect
import os
import pickle
import hashlib
from functools import wraps

# NumPy and Matplotlib: math and plotting
import numpy as np
from numpy.linalg import eigh
import matplotlib.pyplot as plt

# SciPy: probability distributions, math functions, and optimization
# Hint: these tools might be useful later in the project
# ***** MODIFICATION: Added missing imports used by MH & prior/likelihood *****
from scipy.stats import gamma as scipy_gamma
from scipy.stats import norm, wishart as scipy_wishart, multivariate_normal
from scipy.spatial.transform import Rotation as R
from scipy.special import logsumexp, digamma, gammaln # Added gammaln
from scipy.optimize import minimize

# DIPY: diffusion MRI utilities and models
from dipy.io.image import load_nifti, save_nifti   # for loading / saving imaging datasets
from dipy.io.gradients import read_bvals_bvecs     # for loading / saving our bvals and bvecs
from dipy.core.gradients import gradient_table     # for constructing gradient table from bvals/bvecs
from dipy.data import get_fnames                   # for small datasets that we use in tests and examples
from dipy.segment.mask import median_otsu          # for masking out the background
import dipy.reconst.dti as dti                     # for diffusion tensor model fitting and metrics


"""
=============================================================================
Caching Utility (already implemented)
=============================================================================
Provides disk-based memoization to avoid recomputation.
"""

def disk_memoize(cache_dir="cache"):
    """
    Decorator for caching function outputs on disk.

    This utility is already implemented and should not be modified by students.
    It allows expensive computations to be stored and re-used across runs,
    based on the function arguments. If you call the same function again with
    the same inputs, it returns the cached results instead of recomputing.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Optionally force a fresh computation (ignores cache if True)
            force = kwargs.pop("force_recompute", False)

            # Make sure the cache directory exists
            os.makedirs(cache_dir, exist_ok=True)

            # Build a unique hash key from the function name and arguments
            func_name = func.__name__
            key = (func_name, args, kwargs)
            hash_str = hashlib.md5(pickle.dumps(key)).hexdigest()
            cache_path = os.path.join(cache_dir, f"{func_name}_{hash_str}.pkl")

            # Load the cached result if it exists (and recomputation is not forced)
            if not force and os.path.exists(cache_path):
                with open(cache_path, "rb") as f:
                    return pickle.load(f)

            # Otherwise: compute the result, then cache it to disk
            result = func(*args, **kwargs)
            with open(cache_path, "wb") as f:
                pickle.dump(result, f)

            return result

        return wrapper
    return decorator



"""
=============================================================================
Data Loading & Preprocessing (already implemented)
=============================================================================
Loads the Stanford HARDI dataset, applies masking/cropping, and
extracts one voxel with a DTI point estimate for testing.
"""

@disk_memoize()
def get_preprocessed_data():
    """
    Load and preprocess a single voxel of diffusion MRI data.

    What it does:
    - Loads the dataset and gradient information (b-values and b-vectors).
    - Fits a diffusion tensor model (DTI) to one voxel.
    - Extracts a point estimate: baseline signal (S0), eigenvalues, eigenvectors.

    Returns
    -------
    y : ndarray
        Observed diffusion MRI signal vector for a single voxel.
    point_estimate : [S0, evals, evecs]
        Estimated baseline signal, eigenvalues, and eigenvectors.
    gtab : GradientTable
        Gradient table with b-values (diffusion weighting strength)
        and b-vectors (gradient directions).
    """

    # Load the masked data, background mask, and gradient information
    data, mask, gtab = get_data()

    # Initialize a diffusion tensor model (DTI) with S0 estimation enabled
    tenmodel = dti.TensorModel(gtab, return_S0_hat=True)

    # Extract the signal for a single voxel (coordinates chosen for this project)
    # ***** MODIFICATION: Cast y to float like in the notebook *****
    y = data[35, 35, 30, :].astype(float)

    # Fit the DTI model to this voxel's signal
    tenfit = tenmodel.fit(y)

    # Extract point estimates: baseline signal, eigenvalues, and eigenvectors
    # ***** MODIFICATION: Use notebook's fallback logic for robustness *****
    S0 = float(getattr(tenfit, "S0_hat", np.median(y[gtab.bvals <= 50]) if np.any(gtab.bvals<=50) else np.max(y)))
    evals = np.asarray(tenfit.evals)
    evecs = np.asarray(tenfit.evecs)
    point_estimate = [S0, evals, evecs]

    # Return the raw voxel signal, point estimate, and gradient table
    return y, point_estimate, gtab


def get_data():
    """
    Load and preprocess the Stanford HARDI diffusion MRI dataset.

    What it does:
    - Downloads the dataset if not already present (via DIPY).
    - Loads the 4D diffusion MRI volume (x, y, z, measurements).
    - Reads b-values (diffusion weighting strength) and b-vectors (gradient directions).
    - Creates a gradient table (gtab) combining this information.
    - Applies a brain mask and cropping to remove background and reduce size.

    Returns
    -------
    maskdata : ndarray
        The masked and cropped diffusion MRI data.
    mask : ndarray (boolean)
        The brain mask used to exclude background voxels.
    gtab : GradientTable
        Gradient information (b-values and b-vectors) for each measurement.
    """

    # Download filenames for the Stanford HARDI dataset if not already cached
    hardi_fname, hardi_bval_fname, hardi_bvec_fname = get_fnames('stanford_hardi')

    # Load the raw 4D dataset: dimensions are (x, y, z, diffusion measurements)
    data, _ = load_nifti(hardi_fname)

    # Read diffusion weighting information (b-values, b-vectors) and build gradient table
    bvals, bvecs = read_bvals_bvecs(hardi_bval_fname, hardi_bvec_fname)
    gtab = gradient_table(bvals, bvecs)

    # Apply brain masking and cropping to remove background and save compute
    maskdata, mask = median_otsu(
        data, vol_idx=range(10, 50), median_radius=3, numpass=1, autocrop=True, dilate=2
    )

    # Print the final data shape for confirmation
    print('Loaded data with shape: (%d, %d, %d, %d)' % maskdata.shape)

    return maskdata, mask, gtab


"""
=============================================================================
Linear Algebra Helpers (already implemented)
=============================================================================
Functions for reconstructing tensors and switching between
parameterizations. Already implemented.
Hint: you will make use of these helpers later in the project,
the ones involving theta are useful for VI and Laplace.
"""

def compute_D(evals, V):
    """
    Reconstruct the diffusion tensor D from eigenvalues and eigenvectors.

    D = V Λ V.T, where Λ is the diagonal matrix of eigenvalues.

    Parameters
    ----------
    evals : ndarray
        Eigenvalues, shape (3,) or batched.
    V : ndarray
        Eigenvectors, shape (3, 3) or batched.

    Returns
    -------
    D : ndarray
        Diffusion tensor(s), shape (..., 3, 3).
    """

    # ***** MODIFICATION: Use notebook's slightly more robust handling *****
    evals = np.asarray(evals)
    V = np.asarray(V)
    if evals.ndim == 1:
        evals = evals[None, None, :]
    elif evals.ndim == 2:
        evals = evals[:, None, :]
    if V.ndim == 2:
        V = V[None, :, :]

    # Compute D = V Λ V.T as V (V @ Λ).T
    V_scaled = V * evals
    D = np.matmul(V, np.transpose(V_scaled, axes=[0, 2, 1]))

    return D.squeeze() # Use squeeze like in notebook


def theta_from_D(D):
    """
    Convert a diffusion tensor D into an unconstrained parameter vector theta.

    Follows Eq. (18): D = L L.T with L from Cholesky factorization.
    Diagonals are log-transformed, off-diagonals kept raw.

    Parameters
    ----------
    D : ndarray (3, 3)
        Symmetric positive-definite diffusion tensor.

    Returns
    -------
    theta : ndarray (6,)
        Unconstrained parameter vector corresponding to the lower-triangular
        entries of L (log of diagonals, raw off-diagonals).
    """

    # Compute Cholesky factor (lower-triangular L) of D
    L = np.linalg.cholesky(D)

    # Indices of lower-triangular entries (including diagonal)
    p = D.shape[0]
    tril_indices = np.tril_indices(p)
    theta = []

    # Store log of diagonal entries, raw off-diagonal entries
    for i, j in zip(*tril_indices):
        if i == j:
            theta.append(np.log(L[i, j]))   # Diagonal: log-transform
        else:
            theta.append(L[i, j])           # Off-diagonal: raw value

    return np.array(theta)


def D_from_theta(theta):
    """
    Convert unconstrained parameter vector theta back into diffusion tensor D.

    Follows Eq. (18): D = L L.T with L constructed from theta.
    Diagonal entries of L are exponentiated to ensure positivity,
    off-diagonals are used as raw values.

    Parameters
    ----------
    theta : ndarray (..., 6)
        Unconstrained parameters corresponding to the lower-triangular
        entries of L (log-diagonals, raw off-diagonals).

    Returns
    -------
    D : ndarray (..., 3, 3)
        Symmetric positive-definite diffusion tensor(s).
    """

    # Ensure theta is an array and check shape
    theta = np.asarray(theta)
    *batch_shape, _ = theta.shape
    assert theta.shape[-1] == 6, "Last dimension must be 6 for 3x3 lower-triangular matrices."

    # Initialize lower-triangular matrix L
    L = np.zeros((*batch_shape, 3, 3), dtype=theta.dtype)

    # Fill L with exponentiated diagonals and raw off-diagonals
    tril_indices = np.tril_indices(3)
    for k, (i, j) in enumerate(zip(*tril_indices)):
        if i == j:
            L[..., i, j] = np.exp(theta[..., k])   # Diagonal
        else:
            L[..., i, j] = theta[..., k]           # Off-diagonal

    # Reconstruct D = L @ L.T (batch-aware matrix multiplication)
    D = L @ np.swapaxes(L, -1, -2)

    return D.squeeze()


def grad_D_wrt_theta_at_D(D):
    """
    Compute nabla_theta D evaluated at D.

    Uses the parameterization in Eq. (18): D = L L.T where L is built from
    theta. Returns the gradient tensor with one (3x3) slice per theta component.

    Parameters
    ----------
    D : ndarray (3, 3)
        Symmetric positive-definite diffusion tensor.

    Returns
    -------
    grad_D : ndarray (3, 3, 6)
        Gradient of D w.r.t. theta, one 3x3 matrix per parameter.
    """

    # Get Cholesky factor of D and set up indices for lower-triangular entries
    p = D.shape[0]
    L = np.linalg.cholesky(D)
    tril_indices = np.tril_indices(p)
    num_params = len(tril_indices[0])

    # Prepare output container
    grad_D = np.zeros((p, p, num_params))

    # Loop over all parameters in theta
    for k, (m, n) in enumerate(zip(*tril_indices)):

        # Build a basis matrix for the effect of this parameter
        E_mn = np.zeros((p, p))
        if m == n:
            # Diagonal: dL_mm/dtheta = L_mm since L_mm = exp(theta)
            factor = L[m, n]
        else:
            # Off-diagonal: dL_mn/dtheta = 1
            factor = 1.0
        E_mn[m, n] = factor

        # Work out the corresponding change in D
        dD_k = E_mn @ L.T + L @ E_mn.T
        grad_D[:, :, k] = dD_k

    return grad_D

# ***** NEW: Added eigs_from_D from notebook (needed by MH) *****
def eigs_from_D(D):
    D = 0.5*(np.asarray(D) + np.asarray(D).T)
    w, V = np.linalg.eigh(D)
    idx = np.argsort(w)[::-1]
    return w[idx], V[:, idx]

# ***** NEW: Added prior_logpdf_over_D from notebook (needed by MH) *****
def prior_logpdf_over_D(prior, S0, D):
    evals, V = eigs_from_D(D)
    return prior.logpdf(S0, evals, V)

# ***** NEW: Added HYPERS dictionary from notebook *****
HYPERS = dict(
    sigma=29.0,
    alpha_S=2.0,   theta_S=500.0,
    alpha_L=4.0,   theta_L=2.5e-4,
)

"""
=============================================================================
Bayesian Model Components (need to be implemented)
=============================================================================
Students: implement all parts in this section (priors, likelihoods, etc.)
These are required before any inference method can be attempted.
"""

# ***** MODIFICATION: Replaced placeholder with working notebook version *****
class frozen_prior:
    """
    S0 ~ Gamma(alpha_S=2, theta_S=500),
    λ_i iid ~ Gamma(alpha_L=4, theta_L=2.5e-4),
    V ~ Uniform(SO(3)) (log-constant -> ignored).
    """
    def __init__(self):
        self.alpha_S = HYPERS["alpha_S"]; self.theta_S = HYPERS["theta_S"]
        self.alpha_L = HYPERS["alpha_L"]; self.theta_L = HYPERS["theta_L"]

    def logpdf(self, S0, evals, V):
        if S0 <= 0 or np.any(np.asarray(evals) <= 0): return -np.inf # Ensure evals is array for check
        # log p(S0)
        lp = ((self.alpha_S - 1) * np.log(S0)
              - S0 / self.theta_S
              - self.alpha_S * np.log(self.theta_S)
              - gammaln(self.alpha_S))
        # log p(eigenvalues)
        for lam in np.asarray(evals).reshape(3): # Ensure evals is array for iteration
            lp += ((self.alpha_L - 1) * np.log(lam)
                   - lam / self.theta_L
                   - self.alpha_L * np.log(self.theta_L)
                   - gammaln(self.alpha_L))
        return lp  # + const (uniform over SO(3))

    def rvs(self, random_state=None):
        rng = np.random.default_rng() if random_state is None else random_state
        S0 = scipy_gamma.rvs(self.alpha_S, scale=self.theta_S, random_state=rng)
        evals = np.array([scipy_gamma.rvs(self.alpha_L, scale=self.theta_L, random_state=rng) for _ in range(3)])
        V = R.random(random_state=rng).as_matrix()
        return S0, evals, V

# ***** MODIFICATION: Replaced placeholder with working notebook version *****
class frozen_likelihood:
    """
    y_i ~ N( S0 * exp(-q_i^T D q_i), sigma^2 ),  with q_i = sqrt(b_i) g_i
    """
    def __init__(self, y, gtab, point_estimate=None, variance=None, sigma=None): # Added point_estimate, variance to match script structure if needed elsewhere
        self.y = np.asarray(y).astype(float)
        self.gtab = gtab
        # Use sigma from HYPERS if sigma parameter is None
        self.sigma = float(HYPERS["sigma"] if sigma is None else sigma)
        self.sigma2 = self.sigma**2
        self.q = np.sqrt(self.gtab.bvals[:, None]) * self.gtab.bvecs  # (n,3)

    def predict(self, S0, D):
        # This helper was inside logpdf in the notebook, extracted for clarity
        D = np.asarray(D).squeeze()
        assert D.shape == (3,3), f"Expected D (3,3), got {D.shape}"
        quad = np.einsum('ij,jk,ik->i', self.q, D, self.q)
        return S0 * np.exp(-quad)

    def logpdf(self, S0, D=None, evecs=None, evals=None): # Allow passing D or evecs/evals
        # If D is not passed directly, compute it
        if D is None:
             if evecs is None or evals is None:
                 raise ValueError("Must provide either D or both evecs and evals")
             D = compute_D(evals, evecs)

        yhat = self.predict(S0, D)
        n = self.y.size
        const = -0.5 * n * np.log(2*np.pi*self.sigma2)
        return const - 0.5 * np.sum((self.y - yhat)**2) / self.sigma2


"""
=============================================================================
Posterior Approximations (need to be implemented)
=============================================================================
Students: implement these approximations, which are only used in the
corresponding inference methods below:
  - variational_posterior: used only for Variational Inference
  - mvn_reparameterized: used only for Laplace Approximation

They are NOT needed for Metropolis-Hastings or Importance Sampling.
"""


# ==========================
# θ helpers
# ==========================

def pack_theta(S0, theta_D):
    """θ = [log S0, theta_D(6,)] => shape (7,)"""
    return np.concatenate([np.atleast_1d(np.log(S0)), np.asarray(theta_D).reshape(-1)])

def unpack_theta(theta):
    """From θ -> (S0, theta_D (6,), D)"""
    theta = np.asarray(theta)
    assert theta.shape[-1] == 7, "θ must be length 7: [log S0, 6 for D]."
    logS0 = theta[..., 0]
    theta_D = theta[..., 1:]
    S0 = np.exp(logS0)
    D = D_from_theta(theta_D)
    return S0, theta_D, D

def eig_from_D(D):
    """Return evals (λ1..λ3), evecs (3x3) with descending eigenvalues."""
    # np.linalg.eigh returns ascending; flip to descending for conventional DTI
    vals, vecs = np.linalg.eigh(D)
    idx = np.argsort(vals)[::-1]
    vals = vals[idx]
    vecs = vecs[:, idx]
    return vals, vecs


"""
=============================================================================
Inference Methods (need to be implemented)
=============================================================================
Students: implement one method each (MH, IS, VI, or Laplace).
Uses memoization to speed up repeated runs.
"""

# ***** NEW: Added MH helper functions from notebook *****
def project_spd(D):
    D = 0.5*(D + D.T)
    w, V = np.linalg.eigh(D)
    w = np.clip(w, 1e-12, None)
    return V @ np.diag(w) @ V.T

def log_gamma_prop(x, anchor, gamma):
    if x <= 0: return -np.inf
    k = 1.0/(gamma**2); th = (gamma**2)*anchor
    return (k-1)*np.log(x) - x/th - k*np.log(th) - gammaln(k)

def propose_D_rescaled_wishart(D_curr, nu, rng):
    """
    SPEC-compliant base draw + centered-like reparameterization:
      A ~ Wishart(scale=D_curr, df=nu),  D_prop = A/nu.
      log q(D_prop|D_curr) = log Wishart(nu*D_prop; scale=D_curr, df=nu) + d*log(nu),
      with d = p(p+1)/2 = 6 for 3x3 SPD.
    """
    p = 3; d = p*(p+1)//2
    A = scipy_wishart.rvs(df=nu, scale=D_curr, random_state=rng)
    D_prop = project_spd(A / nu)
    log_q = scipy_wishart.logpdf(nu*D_prop, df=nu, scale=D_curr) + d*np.log(nu)
    return D_prop, log_q

def log_q_reverse_rescaled(D_curr, D_prop, nu):
    p = 3; d = p*(p+1)//2
    return scipy_wishart.logpdf(nu*D_curr, df=nu, scale=D_prop) + d*np.log(nu)

#Help functions:

def build_log_posterior_theta(prior, like):
    def logpost(theta):
        S0, _, D = unpack_theta(theta)

        # Ensure D is proper 3x3
        D = np.asarray(D).reshape(3,3)

        # Clamp eigenvalues slightly above 0 to avoid log(0) in priors
        evals, evecs = eig_from_D(D)
        evals = np.maximum(evals, 1e-12)

        # Likelihood
        lp_like = like.logpdf(S0=max(S0, 1e-12), D=D) # Use scalar S0, pass D directly

        # Prior
        # ***** MODIFICATION: Use correct positional arguments *****
        lp_prior = prior.logpdf(max(S0, 1e-12), # Pass S0
                                evals,          # Pass the full evals array
                                evecs)          # Pass V (evecs)

        # Check for non-finite results which can cause optimizer issues
        if not np.isfinite(lp_like + lp_prior):
             return -np.inf # Return negative infinity for invalid states

        return lp_like + lp_prior
    return logpost

def finite_diff_grad(f, x, eps=1e-4):
    x = np.asarray(x, dtype=float)
    g = np.zeros_like(x)
    for i in range(x.size):
        xp = x.copy(); xm = x.copy()
        xp[i] += eps; xm[i] -= eps
        g[i] = (f(xp) - f(xm)) / (2*eps)
    return g


def finite_diff_hessian(f, x, eps=1e-3):
    """
    Symmetric 2nd-order FD Hessian via gradient differences.
    Slightly larger eps than gradient for stability.
    """
    x = np.asarray(x, dtype=float)
    n = x.size
    H = np.zeros((n, n), dtype=float)
    f0 = f(x)
    # Use gradient-based FD for diagonals/off-diagonals
    for i in range(n):
        ei = np.zeros(n); ei[i] = eps
        gi_plus  = finite_diff_grad(f, x + ei, eps=eps/4)
        gi_minus = finite_diff_grad(f, x - ei, eps=eps/4)
        H[:, i] = (gi_plus - gi_minus) / (2*eps)
    # Symmetrize to reduce numerical noise


    #print('H before Symmetrize: ', H)
    H = 0.5*(H + H.T)
    #print('H after Symmetrize: ', H)

    return H


@disk_memoize()
# ***** MODIFICATION: Uncommented and adjusted function signature from notebook *****
def metropolis_hastings(
    n_samples=15000,
    gamma_param=0.01,   # S0 proposal scale (sd ≈ gamma * S0)
    nu_param=60,        # D proposal tightness (after rescale proposals are local); try 40–120
    seed=0,
    trace_every=1000,
    plot_traces=False # Added plot_traces flag from original placeholder
):
    # Students: implement Metropolis-Hastings here.
    # Before starting, make sure the prior and likelihood are implemented.
    # Note: you may change, add, or remove input parameters depending on your design
    # (e.g. pass initialization values like those prepared in main()).

    # ***** MODIFICATION: Copied MH logic from notebook *****
    rng = np.random.default_rng(seed)

    # Data & model (from Block A)
    y, [S0_hat, evals_hat, evecs_hat], gtab = get_preprocessed_data()
    D_hat = compute_D(evals_hat, evecs_hat)
    prior = frozen_prior()
    # ***** MODIFICATION: Pass correct arguments to likelihood *****
    like  = frozen_likelihood(y, gtab, sigma=HYPERS["sigma"])

    # ***** MODIFICATION: Use the correct log posterior function *****
    def log_post(S0, D):
         # ***** MODIFICATION: Ensure S0 is scalar for likelihood call *****
        lp_like = like.logpdf(S0, D=D)
        lp_prior = prior_logpdf_over_D(prior, S0, D)
        return lp_prior + lp_like

    # Initialize at DTI (hint)
    S0_curr = float(max(S0_hat, 1e-9))
    D_curr  = project_spd(D_hat)
    logp_curr = log_post(S0_curr, D_curr)

    # Storage
    S0_samples    = np.zeros(n_samples)
    evals_samples = np.zeros((n_samples, 3))
    evecs_samples = np.zeros((n_samples, 3, 3))

    accepts = 0

    for t in range(n_samples):
        # --- S0' (Gamma) ---
        S0_prop = rng.gamma(shape=1.0/(gamma_param**2), scale=(gamma_param**2)*S0_curr)
        log_qf_S0 = log_gamma_prop(S0_prop, S0_curr, gamma_param)
        log_qr_S0 = log_gamma_prop(S0_curr, S0_prop, gamma_param)

        # --- D' (reparameterized Wishart with Jacobian) ---
        D_prop, log_qf_D = propose_D_rescaled_wishart(D_curr, nu_param, rng)
        log_qr_D = log_q_reverse_rescaled(D_curr, D_prop, nu_param)

        # --- posterior at proposal ---
        logp_prop = log_post(S0_prop, D_prop)

        # --- MH accept/reject ---
        log_alpha = (logp_prop + log_qr_S0 + log_qr_D) - (logp_curr + log_qf_S0 + log_qf_D)

        # Check for NaN/Inf in log_alpha
        if not np.isfinite(log_alpha):
             log_alpha = -np.inf # Reject if proposal leads to invalid state or calculation error

        if np.log(rng.uniform()) < log_alpha:
            S0_curr, D_curr, logp_curr = S0_prop, D_prop, logp_prop
            accepts += 1

        # Save sorted eigens for diagnostics
        w, V = eigs_from_D(D_curr)
        S0_samples[t]    = S0_curr
        evals_samples[t] = w
        evecs_samples[t] = V

        if trace_every and (t+1) % trace_every == 0:
            print(f"[MH] {t+1}/{n_samples}  acc={(accepts/(t+1)):.3f}  logp={logp_curr:.1f}  "
                  f"| gamma={gamma_param}  nu={nu_param}")

    print(f"[MH] Acceptance rate: {accepts/n_samples:.3f}")

    # ***** MODIFICATION: Plotting logic removed, handled by plot_results *****
    # The plot_traces flag is not used here but kept in signature for compatibility

    return S0_samples, evals_samples, evecs_samples


@disk_memoize()
def importance_sampling(n_samples, gamma_param, nu_param):
    # Students: implement Importance Sampling here.
    # Before starting, make sure the prior and likelihood are implemented.
    # Note: you may change, add, or remove input parameters depending on your design
    # (e.g. pass initialization values like those prepared in main()).

    raise NotImplementedError

    return importance_weights, S0_samples, evals_samples, evecs_samples


@disk_memoize()
def variational_inference(max_iters, K, learning_rate):
    # Students: implement Variational Inference here.
    # Before starting, make sure the prior, likelihood and variational_posterior are implemented.
    # Note: you may change, add, or remove input parameters depending on your design
    # (e.g. pass initialization values like those prepared in main()).

    raise NotImplementedError

@disk_memoize()
def laplace_approximation(y, gtab, prior=None, init=None, variance=29, bounds=None):
    raise NotImplementedError


"""
=============================================================================
Visualization & Experiment Runner
=============================================================================
Plotting function and the main() script to run experiments.
"""

def main():
    # Initialize with preprocessed data and DTI point estimate
    y, point_estimate, gtab = get_preprocessed_data()
    S0_init, evals_init, evecs_init = point_estimate
    D_init = compute_D(evals_init, evecs_init).squeeze()

    # Find principal eigenvector from DTI estimate (for plotting)
    evec_principal = evecs_init[:, 0]

    # Set random seed and number of posterior samples
    np.random.seed(0)
    n_samples = 10000

    # === Run Metropolis–Hastings ===
    print("\n--- Running Metropolis-Hastings ---")
    S0_mh, evals_mh, evecs_mh = metropolis_hastings(
        n_samples=15000,
        gamma_param=0.005,
        nu_param=1800,
        seed=0,
        trace_every=1000,
        force_recompute=True # Set to False to use cache
    )
    burn_in = 2000
    plot_results(S0_mh[burn_in:], evals_mh[burn_in:], evecs_mh[burn_in:, :, :], evec_principal, method="mh")
    print("--- Metropolis-Hastings Complete ---")

    '''
    # === Run Importance Sampling (Optional) ===
    # print("\n--- Running Importance Sampling ---")
    # w_is, S0_is, evals_is, evecs_is = importance_sampling(force_recompute=True) # Set to False to use cache
    # plot_results(S0_is, evals_is, evecs_is, evec_principal, weights=w_is, method="is")
    # print("--- Importance Sampling Complete ---")
    

    # === Run Variational Inference ===
    print("\n--- Running Variational Inference ---")
    posterior_vi = variational_inference(max_iters=1000, K=256, learning_rate=1e-4, force_recompute=True) # Set to False to use cache
    S0_vi, evals_vi, evecs_vi = posterior_vi.rvs(size=n_samples)
    plot_results(S0_vi, evals_vi, evecs_vi, evec_principal, method="vi")
    print("--- Variational Inference Complete ---")

    # === Run Laplace Approximation ===
    print("\n--- Running Laplace Approximation ---")
    init = {"S0": S0_init, "evals": evals_init, "evecs": evecs_init}
    posterior_laplace = laplace_approximation( y=y, gtab=gtab, init=init, force_recompute=True) # Set to False to use cache
    S0_laplace, evals_laplace, evecs_laplace = posterior_laplace.rvs(size=n_samples)
    plot_results(S0_laplace, evals_laplace, evecs_laplace, evec_principal, method="laplace")
    print("--- Laplace Approximation Complete ---")
    '''
    print("\nDone.")

def plot_results(S0, evals, evecs, evec_ref, weights=None, method=""):
    """
    Plot posterior results as histograms and save to file.

    Creates histograms of baseline signal (S0), mean diffusivity (MD),
    fractional anisotropy (FA), and the angle between estimated and
    reference eigenvectors.

    Parameters
    ----------
    S0 : ndarray
        Sampled baseline signals.
    evals : ndarray (N, 3)
        Sampled eigenvalues of the diffusion tensor (should be sorted descending).
    evecs : ndarray (N, 3, 3)
        Sampled eigenvectors of the diffusion tensor (columns correspond to evals).
    evec_ref : ndarray (3,)
        Reference principal eigenvector (from point estimate).
    weights : ndarray, optional
        Importance weights for samples. Uniform if None.
    method : str
        Name of inference method (used in output filename).
    """

    # Use uniform weights if none provided
    if weights is None:
        weights = np.ones_like(S0)
    # Normalize weights
    weights /= np.sum(weights)


    # Choose number of bins based on sample size
    n_bins = np.floor(np.sqrt(len(weights))).astype(int)

    # Squeeze arrays for plotting if they have extra dims
    weights = weights.squeeze()
    S0 = S0.squeeze()
    md = dti.mean_diffusivity(evals).squeeze()
    fa = dti.fractional_anisotropy(evals).squeeze()

    # Compute acute angle between estimated principal eigenvector and reference
    # ***** MODIFICATION: Ensure correct principal eigenvector is used (index 0 for descending evals) *****
    v1_samples = evecs[:, :, 0] # Assumes evecs columns correspond to descending evals
    dots = np.einsum('ij,j->i', v1_samples, evec_ref)
    angles = np.degrees(np.arccos(np.clip(np.abs(dots), 0.0, 1.0)))

    # Create 2x2 grid of histograms
    fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Reduced size slightly

    axes[0, 0].hist(S0, bins=n_bins, density=True, weights=weights,
                    alpha=0.7, color='red', edgecolor='black')
    axes[0, 0].set_xlabel("S0")
    axes[0, 0].set_ylabel("Density")
    axes[0, 0].set_title(f"Posterior S0 ({method.upper()})") # Add method name

    axes[0, 1].hist(md, bins=n_bins, density=True, weights=weights,
                    alpha=0.7, color='green', edgecolor='black')
    axes[0, 1].set_xlabel("Mean diffusivity")
    axes[0, 1].set_ylabel("Density")
    axes[0, 1].set_title(f"Posterior MD ({method.upper()})") # Add method name

    axes[1, 0].hist(fa, bins=n_bins, density=True, weights=weights,
                     alpha=0.7, color='blue', edgecolor='black')
    axes[1, 0].set_xlabel("Fractional anisotropy")
    axes[1, 0].set_ylabel("Density")
    axes[1, 0].set_title(f"Posterior FA ({method.upper()})") # Add method name

    axes[1, 1].hist(angles, bins=n_bins, density=True, weights=weights,
                    alpha=0.7, color='magenta', edgecolor='black')
    axes[1, 1].set_xlabel("Acute Angle (degrees)")
    axes[1, 1].set_ylabel("Density")
    axes[1, 1].set_title(f"Posterior Angle ({method.upper()})") # Add method name

    # Adjust layout and save figure with method name
    plt.tight_layout()
    plt.savefig("results_{}.png".format(method), dpi=300, bbox_inches='tight')
    plt.close(fig) # Close figure to avoid displaying it if run as script


if __name__ == "__main__":
    main()