# Standard library: general utilities
import inspect
import os
import pickle
import hashlib
from functools import wraps

# NumPy and Matplotlib: math and plotting
import numpy as np
import matplotlib.pyplot as plt

# SciPy: probability distributions, math functions, and optimization
# Hint: these tools might be useful later in the project
from scipy.stats import gamma, norm, wishart, multivariate_normal
from scipy.spatial.transform import Rotation as R
from scipy.special import logsumexp, digamma
from scipy.optimize import minimize

# DIPY: diffusion MRI utilities and models
from dipy.io.image import load_nifti, save_nifti   # for loading / saving imaging datasets
from dipy.io.gradients import read_bvals_bvecs     # for loading / saving our bvals and bvecs
from dipy.core.gradients import gradient_table     # for constructing gradient table from bvals/bvecs
from dipy.data import get_fnames                   # for small datasets that we use in tests and examples
from dipy.segment.mask import median_otsu          # for masking out the background
import dipy.reconst.dti as dti                     # for diffusion tensor model fitting and metrics



"""
=============================================================================
Caching Utility (already implemented)
=============================================================================
Provides disk-based memoization to avoid recomputation.
"""

def disk_memoize(cache_dir="cache"):
    """
    Decorator for caching function outputs on disk.

    This utility is already implemented and should not be modified by students.
    It allows expensive computations to be stored and re-used across runs,
    based on the function arguments. If you call the same function again with
    the same inputs, it returns the cached results instead of recomputing.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Optionally force a fresh computation (ignores cache if True)
            force = kwargs.pop("force_recompute", False)

            # Make sure the cache directory exists
            os.makedirs(cache_dir, exist_ok=True)

            # Build a unique hash key from the function name and arguments
            func_name = func.__name__
            key = (func_name, args, kwargs)
            hash_str = hashlib.md5(pickle.dumps(key)).hexdigest()
            cache_path = os.path.join(cache_dir, f"{func_name}_{hash_str}.pkl")

            # Load the cached result if it exists (and recomputation is not forced)
            if not force and os.path.exists(cache_path):
                with open(cache_path, "rb") as f:
                    return pickle.load(f)

            # Otherwise: compute the result, then cache it to disk
            result = func(*args, **kwargs)
            with open(cache_path, "wb") as f:
                pickle.dump(result, f)

            return result
        
        return wrapper
    return decorator


"""
=============================================================================
Data Loading & Preprocessing (already implemented)
=============================================================================
Loads the Stanford HARDI dataset, applies masking/cropping, and
extracts one voxel with a DTI point estimate for testing.
"""

@disk_memoize()
def get_preprocessed_data():
    """
    Load and preprocess a single voxel of diffusion MRI data.

    What it does:
    - Loads the dataset and gradient information (b-values and b-vectors).
    - Fits a diffusion tensor model (DTI) to one voxel.
    - Extracts a point estimate: baseline signal (S0), eigenvalues, eigenvectors.

    Returns
    -------
    y : ndarray
        Observed diffusion MRI signal vector for a single voxel.
    point_estimate : [S0, evals, evecs]
        Estimated baseline signal, eigenvalues, and eigenvectors.
    gtab : GradientTable
        Gradient table with b-values (diffusion weighting strength)
        and b-vectors (gradient directions).
    """

    # Load the masked data, background mask, and gradient information
    data, mask, gtab = get_data()

    # Initialize a diffusion tensor model (DTI) with S0 estimation enabled
    tenmodel = dti.TensorModel(gtab, return_S0_hat=True)

    # Extract the signal for a single voxel (coordinates chosen for this project)
    y = data[35, 35, 30, :]

    print('y: ', y)

    # Fit the DTI model to this voxel's signal
    tenfit = tenmodel.fit(y)
    
    # Extract point estimates: baseline signal, eigenvalues, and eigenvectors
    S0 = tenfit.S0_hat
    evals = tenfit.evals
    evecs = tenfit.evecs
    point_estimate = [S0, evals, evecs]

    # Return the raw voxel signal, point estimate, and gradient table
    return S0, evals, evecs, y, point_estimate, gtab 




def get_data():
    """
    Load and preprocess the Stanford HARDI diffusion MRI dataset.

    What it does:
    - Downloads the dataset if not already present (via DIPY).
    - Loads the 4D diffusion MRI volume (x, y, z, measurements).
    - Reads b-values (diffusion weighting strength) and b-vectors (gradient directions).
    - Creates a gradient table (gtab) combining this information.
    - Applies a brain mask and cropping to remove background and reduce size.

    Returns
    -------
    maskdata : ndarray
        The masked and cropped diffusion MRI data.
    mask : ndarray (boolean)
        The brain mask used to exclude background voxels.
    gtab : GradientTable
        Gradient information (b-values and b-vectors) for each measurement.
    """

    # Download filenames for the Stanford HARDI dataset if not already cached 
    hardi_fname, hardi_bval_fname, hardi_bvec_fname = get_fnames('stanford_hardi')

    # Load the raw 4D dataset: dimensions are (x, y, z, diffusion measurements)
    data, _ = load_nifti(hardi_fname)

    # Read diffusion weighting information (b-values, b-vectors) and build gradient table
    bvals, bvecs = read_bvals_bvecs(hardi_bval_fname, hardi_bvec_fname)
    gtab = gradient_table(bvals, bvecs)

    # Apply brain masking and cropping to remove background and save compute
    maskdata, mask = median_otsu(
        data, vol_idx=range(10, 50), median_radius=3, numpass=1, autocrop=True, dilate=2
    )

    # Print the final data shape for confirmation
    print('Loaded data with shape: (%d, %d, %d, %d)' % maskdata.shape)

    return maskdata, mask, gtab


"""
=============================================================================
Linear Algebra Helpers (already implemented)
=============================================================================
Functions for reconstructing tensors and switching between
parameterizations. Already implemented.
Hint: you will make use of these helpers later in the project,
the ones involving theta are useful for VI and Laplace.
"""


# Org function
def compute_D(evals, V):
    """
        Reconstruct the diffusion tensor D from eigenvalues and eigenvectors.

        D = V Λ V.T, where Λ is the diagonal matrix of eigenvalues.

        Parameters
        ----------
        evals : ndarray
            Eigenvalues, shape (3,) or batched.
        V : ndarray
            Eigenvectors, shape (3, 3) or batched.

        Returns
        -------
        D : ndarray
            Diffusion tensor(s), shape (..., 3, 3).
    """
        

    # Ensure inputs have the correct batch dimensions
    if evals.ndim == 1:
        evals = evals[None, None, :]
    elif evals.ndim == 2:
        evals = evals[:, None, :]
    if V.ndim == 2:
        V = V[None, :, :]

    # Compute D = V Λ V.T as V (V @ Λ).T
    V_scaled = V * evals
    D = np.matmul(V, np.transpose(V_scaled, axes=[0, 2, 1]))

    return D




def theta_from_D(D):
    """
    Convert a diffusion tensor D into an unconstrained parameter vector theta.

    Follows Eq. (18): D = L L.T with L from Cholesky factorization.
    Diagonals are log-transformed, off-diagonals kept raw.

    Parameters
    ----------
    D : ndarray (3, 3)
        Symmetric positive-definite diffusion tensor.

    Returns
    -------
    theta : ndarray (6,)
        Unconstrained parameter vector corresponding to the lower-triangular
        entries of L (log of diagonals, raw off-diagonals).
    """
    
    # Compute Cholesky factor (lower-triangular L) of D
    L = np.linalg.cholesky(D)
    
    # Indices of lower-triangular entries (including diagonal)
    p = D.shape[0]
    tril_indices = np.tril_indices(p)
    theta = []

    # Store log of diagonal entries, raw off-diagonal entries
    for i, j in zip(*tril_indices):
        if i == j:
            theta.append(np.log(L[i, j]))   # Diagonal: log-transform
        else:
            theta.append(L[i, j])           # Off-diagonal: raw value

    return np.array(theta)


def D_from_theta(theta):
    """
    Convert unconstrained parameter vector theta back into diffusion tensor D.

    Follows Eq. (18): D = L L.T with L constructed from theta.
    Diagonal entries of L are exponentiated to ensure positivity,
    off-diagonals are used as raw values.

    Parameters
    ----------
    theta : ndarray (..., 6)
        Unconstrained parameters corresponding to the lower-triangular
        entries of L (log-diagonals, raw off-diagonals).

    Returns
    -------
    D : ndarray (..., 3, 3)
        Symmetric positive-definite diffusion tensor(s).
    """
    
    # Ensure theta is an array and check shape
    theta = np.asarray(theta)
    *batch_shape, _ = theta.shape
    assert theta.shape[-1] == 6, "Last dimension must be 6 for 3x3 lower-triangular matrices."

    # Initialize lower-triangular matrix L
    L = np.zeros((*batch_shape, 3, 3), dtype=theta.dtype)

    # Fill L with exponentiated diagonals and raw off-diagonals
    tril_indices = np.tril_indices(3)
    for k, (i, j) in enumerate(zip(*tril_indices)):
        if i == j:
            L[..., i, j] = np.exp(theta[..., k])   # Diagonal
        else:
            L[..., i, j] = theta[..., k]           # Off-diagonal

    # Reconstruct D = L @ L.T (batch-aware matrix multiplication)
    D = L @ np.swapaxes(L, -1, -2)

    return D.squeeze()



def grad_D_wrt_theta_at_D(D):
    """
    Compute nabla_theta D evaluated at D.

    Uses the parameterization in Eq. (18): D = L L.T where L is built from 
    theta. Returns the gradient tensor with one (3x3) slice per theta component.

    Parameters
    ----------
    D : ndarray (3, 3)
        Symmetric positive-definite diffusion tensor.

    Returns
    -------
    grad_D : ndarray (3, 3, 6)
        Gradient of D w.r.t. theta, one 3x3 matrix per parameter.
    """
    
    # Get Cholesky factor of D and set up indices for lower-triangular entries
    p = D.shape[0]
    L = np.linalg.cholesky(D)
    tril_indices = np.tril_indices(p)
    num_params = len(tril_indices[0])

    # Prepare output container
    grad_D = np.zeros((p, p, num_params))

    # Loop over all parameters in theta
    for k, (m, n) in enumerate(zip(*tril_indices)):

        # Build a basis matrix for the effect of this parameter
        E_mn = np.zeros((p, p))
        if m == n:
            # Diagonal: dL_mm/dtheta = L_mm since L_mm = exp(theta)
            factor = L[m, n]
        else:
            # Off-diagonal: dL_mn/dtheta = 1
            factor = 1.0
        E_mn[m, n] = factor

        # Work out the corresponding change in D
        dD_k = E_mn @ L.T + L @ E_mn.T
        grad_D[:, :, k] = dD_k

    return grad_D


"""
=============================================================================
Bayesian Model Components (need to be implemented)
=============================================================================
Students: implement all parts in this section (priors, likelihoods, etc.)
These are required before any inference method can be attempted.
"""

class frozen_prior:
    # Placeholder for the prior distribution.
    # Hint: you may want to add input parameters to these methods.

   
    def __init__(self, sigma=29, alpha_s=2, theta_s=500,alpha_lam=4, theta_lam=2.5e-4):
        # Parameters
        self.sigma = sigma
        self.alpha_s = alpha_s
        self.theta_s = theta_s
        self.alpha_lam = alpha_lam
        self.theta_lam = theta_lam
             
    
    def rvs(self, size=None):
        # Size = None makes it a scalar instead of a size 1 vector
        gamma_s = np.random.gamma(self.alpha_s, self.theta_s, size)
        gamma_lam_1 = np.random.gamma(self.alpha_lam, self.theta_lam, size)
        gamma_lam_2 = np.random.gamma(self.alpha_lam, self.theta_lam, size)
        gamma_lam_3 = np.random.gamma(self.alpha_lam, self.theta_lam, size)
        V = R.random()
        return V, gamma_s, gamma_lam_1, gamma_lam_2, gamma_lam_3 
        
    
    def logpdf(self, gamma_s, gamma_lam_1,gamma_lam_2, gamma_lam_3, V=None):
        """
        x (gamma): The value at which to evaluate the probability density function.

        a (or s, df, c, etc.): This is a distribution-specific shape parameter. For example: 
        In the lognorm distribution, a is the shape parameter. In the gamma distribution, a is the shape parameter. 

        loc: The location parameter, which shifts the distribution's center. For a normal distribution, it corresponds to the mean. 
        
        scale: The scale parameter, which stretches or compresses the distribution. For a normal distribution, it corresponds to the standard deviation. 
        """
        logp_s = gamma.logpdf(gamma_s, a=self.alpha_s, scale=self.theta_s)
        logp_lam_1 = gamma.logpdf(gamma_lam_1, a=self.alpha_lam, scale=self.theta_lam)
        logp_lam_2 = gamma.logpdf(gamma_lam_2, a=self.alpha_lam, scale=self.theta_lam)
        logp_lam_3 = gamma.logpdf(gamma_lam_3, a=self.alpha_lam, scale=self.theta_lam)
        

        # Rotation term (uniform over SO(3), constant, so log prob = 0)
        logp_V = 0.0  
        

        return logp_s + logp_lam_1 + logp_lam_2 + logp_lam_3 + logp_V
   
    
class frozen_likelihood:
    # Placeholder for the likelihood (with partial code provided).
    # Hint: you may want to add input parameters to these methods.

    def __init__(self, gtab,y, point_estimate):
        self.gtab = gtab   # store gradient table with b-values and b-vectors
        self.y = y # added yesterday
        self.point_estimate = point_estimate # added yesterday
    

    def logpdf(self, S0, evecs, evals, variance = 29 ):
        S0 = np.atleast_1d(S0)        # ensure S0 is array-like
        D = compute_D(evals, evecs)   # reconstruct diffusion tensor

        # Build q from diffusion gradients (b-values & b-vectors),
        # corresponds to the experimental setting x in the project instructions
        q = np.sqrt(self.gtab.bvals[:, None]) * self.gtab.bvecs

        # Model signal S given tensor D and baseline S0
        S = S0[:, None] * np.exp( - np.einsum('...j, ijk, ...k->i...', q, D, q))

        logp = norm.logpdf(self.y, loc=S, scale=variance)
        return np.sum(logp)




"""
=============================================================================
Posterior Approximations (need to be implemented)
=============================================================================
Students: implement these approximations, which are only used in the
corresponding inference methods below:
  - variational_posterior: used only for Variational Inference
  - mvn_reparameterized: used only for Laplace Approximation

They are NOT needed for Metropolis-Hastings or Importance Sampling.
"""


# ==========================
# θ helpers 
# ==========================

def pack_theta(S0, theta_D):
    """θ = [log S0, theta_D(6,)] => shape (7,)"""
    return np.concatenate([np.atleast_1d(np.log(S0)), np.asarray(theta_D).reshape(-1)])

def unpack_theta(theta):
    """From θ -> (S0, theta_D (6,), D)"""
    theta = np.asarray(theta)
    assert theta.shape[-1] == 7, "θ must be length 7: [log S0, 6 for D]."
    logS0 = theta[..., 0]
    theta_D = theta[..., 1:]
    S0 = np.exp(logS0)
    D = D_from_theta(theta_D)
    return S0, theta_D, D

def eig_from_D(D):
    """Return evals (λ1..λ3), evecs (3x3) with descending eigenvalues."""
    # np.linalg.eigh returns ascending; flip to descending for conventional DTI
    vals, vecs = np.linalg.eigh(D)
    idx = np.argsort(vals)[::-1]
    vals = vals[idx]
    vecs = vecs[:, idx]
    return vals, vecs



    
    
    
class mvn_reparameterized:

    """
    A Gaussian on θ ~ N(θ̂, Σ), but with helpers to sample/score either in θ-space
    or mapped back to (S0, evals, evecs).
    """
    def __init__(self, mean, cov):
        self.mean = mean
        self.cov = cov
        self.dist = multivariate_normal(mean=mean, cov=cov)

    def rvs(self, size=1):
        # Draw sample of theata after reparameterization
        thetas = self.dist.rvs(size=size)
        if thetas.ndim == 1:
            thetas = thetas[None, :]

        # go back to original params
        S0_samples = np.exp(thetas[:, 0])   # since theta_1 = log S0
        D_values = np.array([D_from_theta(theta[1:]) for theta in thetas])  # D_from_theta given as a help function by teacher
        evals_samples = np.linalg.eigvalsh(D_values)
        evecs_samples = np.array([np.linalg.eigh(Ds)[1] for Ds in D_values])
        return S0_samples, evals_samples, evecs_samples

    def logpdf(self, theta):
        return self.dist.logpdf(theta)
    

    

"""
=============================================================================
Inference Methods (need to be implemented)
=============================================================================
Students: implement one method each (MH, IS, VI, or Laplace).
Uses memoization to speed up repeated runs.
"""
#Help functions:

def build_log_posterior_theta(prior, like):
    def logpost(theta):
        S0, _, D = unpack_theta(theta)

        # Ensure D is proper 3x3
        D = np.asarray(D).reshape(3,3)

        # Clamp eigenvalues slightly above 0 to avoid log(0) in priors
        evals, evecs = eig_from_D(D)
        evals = np.maximum(evals, 1e-12)

        # Likelihood
        lp_like = like.logpdf(S0=np.atleast_1d(max(S0, 1e-12)),
                              evecs=evecs,
                              evals=evals)

        # Prior
        lp_prior = prior.logpdf(gamma_s=max(S0, 1e-12),
                                gamma_lam_1=evals[0],
                                gamma_lam_2=evals[1],
                                gamma_lam_3=evals[2],
                                V=evecs)

        return lp_like + lp_prior
    return logpost

def finite_diff_grad(f, x, eps=1e-4):
    x = np.asarray(x, dtype=float)
    g = np.zeros_like(x)
    for i in range(x.size):
        xp = x.copy(); xm = x.copy()
        xp[i] += eps; xm[i] -= eps
        g[i] = (f(xp) - f(xm)) / (2*eps)
    return g


def finite_diff_hessian(f, x, eps=1e-3):
    """
    Symmetric 2nd-order FD Hessian via gradient differences.
    Slightly larger eps than gradient for stability.
    """
    x = np.asarray(x, dtype=float)
    n = x.size
    H = np.zeros((n, n), dtype=float)
    f0 = f(x)
    # Use gradient-based FD for diagonals/off-diagonals
    for i in range(n):
        ei = np.zeros(n); ei[i] = eps
        gi_plus  = finite_diff_grad(f, x + ei, eps=eps/4)
        gi_minus = finite_diff_grad(f, x - ei, eps=eps/4)
        H[:, i] = (gi_plus - gi_minus) / (2*eps)
    # Symmetrize to reduce numerical noise

    
    print('H before Symmetrize: ', H)
    H = 0.5*(H + H.T)
    print('H after Symmetrize: ', H)

    return H


@disk_memoize()
def laplace_approximation(y, gtab, prior=None, init=None, variance=29, bounds=None):
    """
    Fits a Laplace approximation in θ-space and returns mvn_reparameterized.

    Parameters
    ----------
    y, gtab : from data pipeline
    prior : frozen_prior instance 
    init  : optional dict with 'S0', 'evals', 'evecs' for initialization
    variance : noise SD for likelihood (kept aligned with your frozen_likelihood default)
    bounds : optional box constraints for θ (L-BFGS-B). If None, keep unconstrained.

    Returns
    -------
    mvn_reparameterized(theta_hat, Sigma)
    """
    if prior is None:
        prior = frozen_prior(sigma=variance)

    like = frozen_likelihood(gtab=gtab, y=y, point_estimate=None)

    # Initialization
  
    S0_0 = float(init['S0'])
    evals_0 = np.asarray(init['evals'])
    evecs_0 = np.asarray(init['evecs'])

    D0 = compute_D(evals_0, evecs_0)
    D0 = np.asarray(D0).reshape(3,3)       #ensure 3x3
    theta_D0 = theta_from_D(D0)
    theta0 = pack_theta(max(S0_0, 1e-12), theta_D0)  # floor S0

    assert theta0.shape == (7,), f"Got wrong theta0 shape: {theta0.shape}" # Extra check


    logpost = build_log_posterior_theta(prior, like)

    def nlp(theta):
        return -logpost(theta)

    def nlp_grad(theta):
        return -finite_diff_grad(logpost, theta, eps=1e-4)

    # (Optional) simple bounds—often not necessary in theta space; left None by default
    res = minimize(nlp, theta0, method='L-BFGS-B', jac=nlp_grad, bounds=bounds, options=dict(maxiter=500, ftol=1e-9, gtol=1e-7))

    theta_hat = res.x

    # Hessian at the mode (of -logpost); Σ = H⁻¹ where H = ∇²[-logpost](θ̂)
    H = finite_diff_hessian(lambda th: -logpost(th), theta_hat, eps=1e-3)
    print('H: ', H)

    # Regularize if needed (numerical guard)
    # Ensure positive-definite by adding a tiny ridge if necessary
    try:
        # Try Cholesky to test PD-ness; add tiny jitter if it fails
        np.linalg.cholesky(H)
    except np.linalg.LinAlgError:
        H = H + 1e-6 * np.eye(H.shape[0])

    Sigma = np.linalg.inv(H)

    return mvn_reparameterized(mean=theta_hat, cov=Sigma)



# --------------------------------------------------------------------------
# Laplace Approximation
# --------------------------------------------------------------------------

@disk_memoize()
def laplace_approximation(
    wishart_df=None,
    wishart_scale=None,
    gamma_alpha_s=2.0,
    gamma_theta_s=500.0,
    obs_sigma=29.0,
    hess_eps=1e-4,
    jitter=1e-6,
    verbose=True,
):
    """
    Builds a Laplace posterior q(theta)=N(theta_hat, Sigma) with:
      - theta = [log S0, theta_L] where D = L L^T and L uses Eq.(18) parameterization
      - prior: S0 ~ Gamma(gamma_alpha_s, scale=gamma_theta_s)
               D ~ Wishart(df=wishart_df, scale=wishart_scale)
      - likelihood: y_i ~ N(S0 * exp(-q_i^T D q_i), obs_sigma^2)

    Returns
    -------
    approx : mvn_reparameterized
        Distribution-like object with rvs() and logpdf().
    """
    # ---- data & a sensible Wishart prior centered near DTI estimate ----
    S0_hat, evals_hat, evecs_hat, y, point_estimate, gtab = get_preprocessed_data()
    D_hat = compute_D(evals_hat, evecs_hat).squeeze()
    p = 3

    if wishart_df is None:
        wishart_df = p + 1  # weakly-informative minimal df
    if wishart_scale is None:
        # Use the DTI fit as a rough prior scale (acts like a prior "mean-ish")
        wishart_scale = D_hat

    # cache q = sqrt(b) * bvecs
    q = np.sqrt(gtab.bvals[:, None]) * gtab.bvecs

    # ---- helpers: prior, likelihood, posterior in theta-space ----
    def log_prior_theta(theta):
        S0, D = mvn_reparameterized._theta_to_S0_D(theta)
        # Gamma prior on S0 (in S0-space; transform taken care of by using theta=log S0)
        lp_S0 = gamma.logpdf(S0, a=gamma_alpha_s, scale=gamma_theta_s) + np.log(S0)  # +log|dS0/dθ0|=log S0

        # Wishart prior on D
        # scipy's wishart.logpdf expects symmetric PD; use 'scale' parameterization
        lp_D = wishart.logpdf(D, df=wishart_df, scale=wishart_scale)

        return lp_S0 + lp_D

    def log_lik_theta(theta):
        S0, D = mvn_reparameterized._theta_to_S0_D(theta)
        # model S = S0 * exp( - q^T D q )
        expo = -np.einsum('ij, jk, ik->i', q, D, q)
        S = S0 * np.exp(expo)
        return np.sum(norm.logpdf(y, loc=S, scale=obs_sigma))

    def log_post(theta):
        return log_prior_theta(theta) + log_lik_theta(theta)

    # ---- initialize theta at a sane point (close to DTI fit) ----
    theta_L0 = theta_from_D(D_hat)           # length-6 vector (Eq.18 order; you already provided this)
    theta0 = np.concatenate([np.log([S0_hat]), theta_L0])  # length 7

    # ---- optimize (maximize log posterior) ----
    def neg_log_post(theta):
        # scipy.minimize minimizes, so flip sign
        return -log_post(theta)

    res = minimize(
        neg_log_post,
        theta0,
        method="L-BFGS-B",
        options=dict(maxiter=500, ftol=1e-9),
    )

    if not res.success and verbose:
        print("[Laplace] Warning: optimizer did not fully converge:", res.message)

    theta_hat = res.x

    # ---- numerical Hessian of log posterior at the mode (central differences) ----
    def central_grad(f, x, eps=hess_eps):
        x = np.asarray(x)
        g = np.zeros_like(x)
        for i in range(x.size):
            e = np.zeros_like(x); e[i] = eps
            g[i] = (f(x + e) - f(x - e)) / (2.0 * eps)
        return g

    def central_hess(f, x, eps=hess_eps):
        x = np.asarray(x)
        n = x.size
        H = np.zeros((n, n))
        # Use central differences on each pair (i,j)
        for i in range(n):
            ei = np.zeros(n); ei[i] = eps
            for j in range(i, n):
                ej = np.zeros(n); ej[j] = eps
                fpp = f(x + ei + ej)
                fpm = f(x + ei - ej)
                fmp = f(x - ei + ej)
                fmm = f(x - ei - ej)
                H_ij = (fpp - fpm - fmp + fmm) / (4.0 * eps * eps)
                H[i, j] = H_ij
                H[j, i] = H_ij
        return H

    # Hessian of log posterior -> we need -H to invert (since Sigma = (-∇^2 log p)^-1)
    H_logpost = central_hess(log_post, theta_hat, eps=hess_eps)

    # Ensure positive definiteness of (-H) with a small jitter
    neg_H = -H_logpost
    # Symmetrize for safety
    neg_H = 0.5 * (neg_H + neg_H.T)
    neg_H += jitter * np.eye(neg_H.shape[0])

    try:
        # Prefer Cholesky-based inverse for stability
        Lc = np.linalg.cholesky(neg_H)
        Sigma = np.linalg.solve(Lc.T, np.linalg.solve(Lc, np.eye(neg_H.shape[0])))
    except np.linalg.LinAlgError:
        if verbose:
            print("[Laplace] Cholesky failed, falling back to np.linalg.pinv.")
        Sigma = np.linalg.pinv(neg_H)

    # ---- package result as a distribution-like object ----
    prior_hypers = dict(
        gamma_alpha_s=gamma_alpha_s,
        gamma_theta_s=gamma_theta_s,
        wishart_df=wishart_df,
        wishart_scale=wishart_scale,
        obs_sigma=obs_sigma,
    )
    approx = mvn_reparameterized(theta_hat, Sigma, gtab, y, prior_hypers, q_cache=q)

    if verbose:
        S0_mode, D_mode = approx._theta_to_S0_D(theta_hat)
        evals_mode, _ = approx._D_to_eigendecomp(D_mode)
        print("[Laplace] Done.")
        print(f"  theta_hat: {theta_hat}")
        print(f"  S0_mode: {S0_mode:.3f}")
        print(f"  evals_mode: {evals_mode}")

    return approx



"""
=============================================================================
Visualization & Experiment Runner
=============================================================================
Plotting function and the main() script to run experiments.
"""

def main():
    # Initialize with preprocessed data and DTI point estimate
    # (these values can be used as starting points for inference methods)
    S0, evals, evecs, y, point_estimate, gtab  = get_preprocessed_data(force_recompute=False)
    S0_init, evals_init, evecs_init = point_estimate
    D_init = compute_D(evals_init, evecs_init).squeeze()
    
    # Find principal eigenvector from DTI estimate (for plotting)
    evec_principal = evecs_init[:, 0]

    # Set random seed and number of posterior samples
    np.random.seed(0)
    n_samples = 10000

   
    #Other functions:
    """
     # Run Metropolis–Hastings and plot results
    S0_mh, evals_mh, evecs_mh = metropolis_hastings(force_recompute=False)
    burn_in = 0
    plot_results(S0_mh[burn_in:], evals_mh[burn_in:], evecs_mh[burn_in:, :, :], evec_principal, method="mh")

    # Run Importance Sampling and plot results
    w_is, S0_is, evals_is, evecs_is = importance_sampling(force_recompute=False)
    plot_results(S0_is, evals_is, evecs_is, evec_principal, weights=w_is, method="is")

    # Run Variational Inference and plot results
    posterior_vi = variational_inference(force_recompute=False)
    S0_vi, evals_vi, evecs_vi = posterior_vi.rvs(size=n_samples)
    plot_results(S0_vi, evals_vi, evecs_vi, evec_principal, method="vi")

    """

    # Run Laplace Approximation and plot results

    S0, evals, evecs, y, point_estimate, gtab = get_preprocessed_data()
    init = {"S0": S0, "evals": evals, "evecs": evecs}

    posterior_laplace = laplace_approximation( y=y, gtab=gtab, init=init, force_recompute=False)
    S0_laplace, evals_laplace, evecs_laplace = posterior_laplace.rvs(size=n_samples)
    plot_results(S0_laplace, evals_laplace, evecs_laplace, evec_principal, method="laplace")

    print("Done.")


def plot_results(S0, evals, evecs, evec_ref, weights=None, method=""):
    """
    Plot posterior results as histograms and save to file.

    Creates histograms of baseline signal (S0), mean diffusivity (MD),
    fractional anisotropy (FA), and the angle between estimated and
    reference eigenvectors.

    Parameters
    ----------
    S0 : ndarray
        Sampled baseline signals.
    evals : ndarray
        Sampled eigenvalues of the diffusion tensor.
    evecs : ndarray
        Sampled eigenvectors of the diffusion tensor.
    evec_ref : ndarray
        Reference principal eigenvector (from point estimate).
    weights : ndarray, optional
        Importance weights for samples. Uniform if None.
    method : str
        Name of inference method (used in output filename).
    """
    
    # Use uniform weights if none provided
    if weights is None:
        weights = np.ones_like(S0)
        weights /= np.sum(weights)

    # Choose number of bins based on sample size
    n_bins = np.floor(np.sqrt(len(weights))).astype(int)

    # Squeeze arrays for plotting
    weights = weights.squeeze()
    S0 = S0.squeeze()
    md = dti.mean_diffusivity(evals).squeeze()
    fa = dti.fractional_anisotropy(evals).squeeze()

    # Compute acute angle between estimated and reference eigenvectors
    angle = 360/(2*np.pi) * np.arccos(np.abs(np.dot(evecs[:, :, 2], evec_ref)))
    
    # Create 2x2 grid of histograms
    fig, axes = plt.subplots(2, 2, figsize=(12, 12), sharey=False)

    axes[0, 0].hist(S0, bins=n_bins, density=True, weights=weights, 
                    alpha=0.7, color='red', edgecolor='black')
    axes[0, 0].set_xlabel("S0")
    axes[0, 0].set_ylabel("Density")

    axes[0, 1].hist(md, bins=n_bins, density=True, weights=weights, 
                    alpha=0.7, color='green', edgecolor='black')
    axes[0, 1].set_xlabel("Mean diffusivity")
    axes[0, 1].set_ylabel("Density")

    axes[1, 0].hist(fa, bins=n_bins, density=True, weights=weights,
                     alpha=0.7, color='blue', edgecolor='black')
    axes[1, 0].set_xlabel("Fractional anisotropy")
    axes[1, 0].set_ylabel("Density")

    axes[1, 1].hist(angle, bins=n_bins, density=True, weights=weights, 
                    alpha=0.7, color='magenta', edgecolor='black')
    axes[1, 1].set_xlabel("Acute angle")
    axes[1, 1].set_ylabel("Density")

    # Adjust layout and save figure with method name
    plt.tight_layout()
    plt.savefig("results_{}.png".format(method), dpi=300, bbox_inches='tight')

    plt.show()


if __name__ == "__main__":
    main()